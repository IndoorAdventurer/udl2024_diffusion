{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffusion + WGAN experiments\n",
    "This is the main script for experiments related to combining diffusion with\n",
    "WGAN-based loss within the same training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vincent/miniforge3/envs/pytorch_base/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "\n",
    "# pretend we are in the root folder:\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from udl_module.datasets import dataset_factory\n",
    "from udl_module.nn import unet_factory\n",
    "from udl_module.vincent_diffusion import DiffusionModel\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR10, MNIST\n",
    "\n",
    "import lightning as L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Development\n",
    "Will paste stuff above once it works :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the datasets:\n",
    "\n",
    "trainset, testset = dataset_factory(CIFAR10)\n",
    "\n",
    "batch_size = 32\n",
    "dl_kwargs = {\n",
    "    \"batch_size\": batch_size, \"shuffle\": True, \"num_workers\": 4, \"pin_memory\": True,\n",
    "}\n",
    "\n",
    "trainloader = DataLoader(\n",
    "    trainset, **dl_kwargs\n",
    ")\n",
    "testloader = DataLoader(\n",
    "    testset, **dl_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "# From: https://github.com/Zeleni9/pytorch-wgan/blob/e594e2eef7dbd82d6ad23e9442006f6aee08db6e/models/wgan_gradient_penalty.py\n",
    "class Generator(torch.nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        # Filters [1024, 512, 256]\n",
    "        # Input_dim = 100\n",
    "        # Output_dim = C (number of channels)\n",
    "        self.main_module = nn.Sequential(\n",
    "            # Z latent vector 100\n",
    "            nn.ConvTranspose2d(in_channels=100, out_channels=1024, kernel_size=4, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(num_features=1024),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            # State (1024x4x4)\n",
    "            nn.ConvTranspose2d(in_channels=1024, out_channels=512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(num_features=512),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            # State (512x8x8)\n",
    "            nn.ConvTranspose2d(in_channels=512, out_channels=256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(num_features=256),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            # State (256x16x16)\n",
    "            nn.ConvTranspose2d(in_channels=256, out_channels=channels, kernel_size=4, stride=2, padding=1))\n",
    "            # output of main module --> Image (Cx32x32)\n",
    "\n",
    "        self.output = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.main_module(x)\n",
    "        return self.output(x)\n",
    "\n",
    "class Discriminator(torch.nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        # Filters [256, 512, 1024]\n",
    "        # Input_dim = channels (Cx64x64)\n",
    "        # Output_dim = 1\n",
    "        self.main_module = nn.Sequential(\n",
    "            # Omitting batch normalization in critic because our new penalized training objective (WGAN with gradient penalty) is no longer valid\n",
    "            # in this setting, since we penalize the norm of the critic's gradient with respect to each input independently and not the enitre batch.\n",
    "            # There is not good & fast implementation of layer normalization --> using per instance normalization nn.InstanceNorm2d()\n",
    "            # Image (Cx32x32)\n",
    "            nn.Conv2d(in_channels=channels, out_channels=256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(256, affine=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # State (256x16x16)\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(512, affine=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # State (512x8x8)\n",
    "            nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=4, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(1024, affine=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True))\n",
    "            # output of main module --> State (1024x4x4)\n",
    "\n",
    "        self.output = nn.Sequential(\n",
    "            # The output of D is no longer a probability, we do not apply sigmoid at the output of D.\n",
    "            nn.Conv2d(in_channels=1024, out_channels=1, kernel_size=4, stride=1, padding=0))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.main_module(x)\n",
    "        return self.output(x)\n",
    "\n",
    "    def feature_extraction(self, x):\n",
    "        # Use discriminator for feature extraction then flatten to vector of 16384\n",
    "        x = self.main_module(x)\n",
    "        return x.view(-1, 1024*4*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WGANWithGradientPenaltyFuncs:\n",
    "    \"\"\"Encapsulates all computations specific to the Wasserstein GAN with\n",
    "    Gradient Penalty instead of weight clipping.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, gp_weight: float = 10, critic_iterations: int = 5):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            gp_weight: scale factor for gradient penalty term in critic loss.\n",
    "            critic_iterations: this many critic learning steps for every one\n",
    "                generator learning step. -1 Means never called at all.\n",
    "        \"\"\"\n",
    "        self.gp_weight = gp_weight\n",
    "        self.critic_iterations = critic_iterations\n",
    "    \n",
    "    def also_train_generator(self, batch_idx):\n",
    "        \"\"\"Given the batch index, checks if it is time to train the generator\n",
    "        again.\"\"\"\n",
    "        if self.critic_iterations == -1:\n",
    "            return False\n",
    "        return (batch_idx % self.critic_iterations) == 0\n",
    "    \n",
    "    def critic_loss(self, critic_model, batch, fake_imgs):\n",
    "        \"\"\"Returns loss for the critic model.\"\"\"\n",
    "        fake_imgs = fake_imgs.detach() # just to be sure :-p\n",
    "\n",
    "        critic_real = critic_model(batch)\n",
    "        critic_fake = critic_model(fake_imgs)\n",
    "        grad_pen = self.compute_gradient_penalty(critic_model, batch, fake_imgs)\n",
    "\n",
    "        # Calculating the Wasserstein loss:\n",
    "        return (\n",
    "            - torch.mean(critic_real)\n",
    "            + torch.mean(critic_fake)\n",
    "            + self.gp_weight * grad_pen\n",
    "        )\n",
    "    \n",
    "    def generator_loss(self, critic_model, fake_imgs):\n",
    "        \"\"\"Returns loss for the generator. Should only be called when\n",
    "        `also_train_generator()` returns `True`\n",
    "        \"\"\"\n",
    "        critic_fake = critic_model(fake_imgs)\n",
    "        return -torch.mean(critic_fake)\n",
    "\n",
    "    def compute_gradient_penalty(self, critic_model, real, fake):\n",
    "        # Get critic outputs on random mix of real and fake images\n",
    "        alpha = torch.rand(real.shape[0], *([1] * (len(real.shape) - 1)),\n",
    "                           device=real.device)\n",
    "        mixed = alpha * real + (1 - alpha) * fake\n",
    "        mixed.requires_grad_(True)\n",
    "        critic_mixed = critic_model(mixed)\n",
    "\n",
    "        # Compute gradients of \n",
    "        gradients = torch.autograd.grad(\n",
    "            inputs=mixed,\n",
    "            outputs=critic_mixed,\n",
    "            grad_outputs=torch.ones_like(critic_mixed),\n",
    "            create_graph=True, retain_graph=True\n",
    "        )[0].view(real.shape[0], -1)\n",
    "\n",
    "        # Returning the gradient penalty:\n",
    "        return (torch.mean(gradients.norm(2, dim=1) - 1) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name | Type          | Params | Mode \n",
      "-----------------------------------------------\n",
      "0 | gen  | Generator     | 12.1 M | train\n",
      "1 | cri  | Discriminator | 10.5 M | train\n",
      "-----------------------------------------------\n",
      "22.7 M    Trainable params\n",
      "0         Non-trainable params\n",
      "22.7 M    Total params\n",
      "90.647    Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 782/782 [01:29<00:00,  8.73it/s, train/loss_critic=-12.9, train/loss_generator=31.00]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 782/782 [01:29<00:00,  8.73it/s, train/loss_critic=-12.9, train/loss_generator=31.00]\n"
     ]
    }
   ],
   "source": [
    "class WGANWithGradientPenalty(L.LightningModule):\n",
    "    \"\"\"Wasserstein Gan that uses Gradient Penalty instead of weight clipping.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            generator: nn.Module,\n",
    "            critic: nn.Module,\n",
    "            gp_weight: float = 10,\n",
    "            critic_iterations: int = 5,\n",
    "            optimizer_cls: type[torch.optim.Optimizer] = torch.optim.Adam,\n",
    "            optimizer_args: dict = {\"lr\": 1e-4, \"betas\": (0.5, 0.99)}\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.gen = generator\n",
    "        self.cri = critic\n",
    "        self.wgan_funcs = WGANWithGradientPenaltyFuncs(gp_weight, critic_iterations)\n",
    "        self.optimizer_cls = optimizer_cls\n",
    "        self.optimizer_args = optimizer_args\n",
    "\n",
    "        self.automatic_optimization = False\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.gen(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        opt_g, opt_c = self.optimizers()\n",
    "\n",
    "        self.toggle_optimizer(opt_c)\n",
    "\n",
    "        # Creating fake images\n",
    "        with torch.no_grad():\n",
    "            noise = torch.randn((batch.shape[0], 100, 1, 1), device=self.device)\n",
    "            fake_imgs = self.gen(noise)\n",
    "        \n",
    "        loss_critic = self.wgan_funcs.critic_loss(self.cri, batch, fake_imgs)\n",
    "\n",
    "        # Weight update:\n",
    "        opt_c.zero_grad()\n",
    "        self.manual_backward(loss_critic)\n",
    "        opt_c.step()\n",
    "        self.log(\"train/loss_critic\", loss_critic, prog_bar=True)\n",
    "\n",
    "        self.untoggle_optimizer(opt_c) # not really sure if needed but well..\n",
    "        \n",
    "        also_train_generator = self.wgan_funcs.also_train_generator(batch_idx)\n",
    "\n",
    "        if also_train_generator:\n",
    "            self.toggle_optimizer(opt_g)\n",
    "\n",
    "            # Generate images (TODO: maybe can re-use from discriminator step)\n",
    "            noise = torch.randn((batch.shape[0], 100, 1, 1), device=self.device)\n",
    "            fake_imgs = self.gen(noise)\n",
    "            # critic_fake = self.cri(fake_imgs)\n",
    "            # loss_gen = - torch.mean(critic_fake)\n",
    "            loss_gen = self.wgan_funcs.generator_loss(self.cri, fake_imgs)\n",
    "\n",
    "            # Weight update:\n",
    "            opt_g.zero_grad()\n",
    "            self.manual_backward(loss_gen)\n",
    "            opt_g.step()\n",
    "            \n",
    "            self.log(\"train/loss_generator\", loss_gen, prog_bar=True)\n",
    "\n",
    "            self.untoggle_optimizer(opt_g)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        opt_g = self.optimizer_cls(self.gen.parameters(), **self.optimizer_args)\n",
    "        opt_c = self.optimizer_cls(self.cri.parameters(), **self.optimizer_args)\n",
    "        return opt_g, opt_c\n",
    "\n",
    "import numpy as np\n",
    "torch.manual_seed(1996)\n",
    "np.random.seed(1996)\n",
    "\n",
    "# Creating the datasets:\n",
    "trainset, testset = dataset_factory(CIFAR10)\n",
    "\n",
    "batch_size = 64\n",
    "dl_kwargs = {\n",
    "    \"batch_size\": batch_size, \"shuffle\": True, \"num_workers\": 4, \"pin_memory\": True,\n",
    "}\n",
    "\n",
    "trainloader = DataLoader(\n",
    "    trainset, **dl_kwargs\n",
    ")\n",
    "\n",
    "img_size = (32,32,3)\n",
    "\n",
    "wgan = WGANWithGradientPenalty(\n",
    "    Generator(3),\n",
    "    Discriminator(3)\n",
    ")\n",
    "\n",
    "# Experiment\n",
    "trainer = L.Trainer(\n",
    "    max_time={\"minutes\": 5},\n",
    "    max_epochs=1,\n",
    "    # max_steps=100,\n",
    "    enable_checkpointing=False,\n",
    "    logger=False,\n",
    "    enable_progress_bar=True,\n",
    "    # accelerator=\"cpu\",\n",
    "    # fast_dev_run=True\n",
    ")\n",
    "\n",
    "trainer.fit(wgan, trainloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "83/782 [00:09<01:19,  8.81it/s, train/loss_critic=-35.4, train/loss_generator=44.40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = torch.randn((5, 100, 1, 1))\n",
    "fake_imgs = wgan.gen(noise).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x79fa372ad5e0>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(fake_imgs[4].transpose(0,2) * 0.5 + 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.5153, 0.5153, 0.5153],\n",
      "         [0.5153, 0.5153, 0.5153],\n",
      "         [0.5153, 0.5153, 0.5153]],\n",
      "\n",
      "        [[0.3762, 0.3762, 0.3762],\n",
      "         [0.3762, 0.3762, 0.3762],\n",
      "         [0.3762, 0.3762, 0.3762]],\n",
      "\n",
      "        [[0.5448, 0.5448, 0.5448],\n",
      "         [0.5448, 0.5448, 0.5448],\n",
      "         [0.5448, 0.5448, 0.5448]],\n",
      "\n",
      "        [[0.3412, 0.3412, 0.3412],\n",
      "         [0.3412, 0.3412, 0.3412],\n",
      "         [0.3412, 0.3412, 0.3412]],\n",
      "\n",
      "        [[0.7485, 0.7485, 0.7485],\n",
      "         [0.7485, 0.7485, 0.7485],\n",
      "         [0.7485, 0.7485, 0.7485]]])\n"
     ]
    }
   ],
   "source": [
    "test_x = torch.ones((5,3,3))\n",
    "\n",
    "test_y = torch.rand(5,1,1)\n",
    "\n",
    "print(test_x * test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import imageio\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from torchvision.utils import make_grid\n",
    "# from torch.autograd import Variable\n",
    "# from torch.autograd import grad as torch_grad\n",
    "\n",
    "\n",
    "# class Trainer():\n",
    "#     def __init__(self, generator, discriminator, gen_optimizer, dis_optimizer,\n",
    "#                  gp_weight=10, critic_iterations=5, print_every=50,\n",
    "#                  use_cuda=False):\n",
    "#         self.G = generator\n",
    "#         self.G_opt = gen_optimizer\n",
    "#         self.D = discriminator\n",
    "#         self.D_opt = dis_optimizer\n",
    "#         self.losses = {'G': [], 'D': [], 'GP': [], 'gradient_norm': []}\n",
    "#         self.num_steps = 0\n",
    "#         self.use_cuda = use_cuda\n",
    "#         self.gp_weight = gp_weight\n",
    "#         self.critic_iterations = critic_iterations\n",
    "#         self.print_every = print_every\n",
    "\n",
    "#         if self.use_cuda:\n",
    "#             self.G.cuda()\n",
    "#             self.D.cuda()\n",
    "\n",
    "#     def _critic_train_iteration(self, data):\n",
    "#         \"\"\" \"\"\"\n",
    "#         # Get generated data\n",
    "#         batch_size = data.size()[0]\n",
    "#         generated_data = self.sample_generator(batch_size)\n",
    "\n",
    "#         # Calculate probabilities on real and generated data\n",
    "#         data = Variable(data)\n",
    "#         if self.use_cuda:\n",
    "#             data = data.cuda()\n",
    "#         d_real = self.D(data)\n",
    "#         d_generated = self.D(generated_data)\n",
    "\n",
    "#         # Get gradient penalty\n",
    "#         gradient_penalty = self._gradient_penalty(data, generated_data)\n",
    "#         self.losses['GP'].append(gradient_penalty.data[0])\n",
    "\n",
    "#         # Create total loss and optimize\n",
    "#         self.D_opt.zero_grad()\n",
    "#         d_loss = d_generated.mean() - d_real.mean() + gradient_penalty\n",
    "#         d_loss.backward()\n",
    "\n",
    "#         self.D_opt.step()\n",
    "\n",
    "#         # Record loss\n",
    "#         self.losses['D'].append(d_loss.data[0])\n",
    "\n",
    "#     def _generator_train_iteration(self, data):\n",
    "#         \"\"\" \"\"\"\n",
    "#         self.G_opt.zero_grad()\n",
    "\n",
    "#         # Get generated data\n",
    "#         batch_size = data.size()[0]\n",
    "#         generated_data = self.sample_generator(batch_size)\n",
    "\n",
    "#         # Calculate loss and optimize\n",
    "#         d_generated = self.D(generated_data)\n",
    "#         g_loss = - d_generated.mean()\n",
    "#         g_loss.backward()\n",
    "#         self.G_opt.step()\n",
    "\n",
    "#         # Record loss\n",
    "#         self.losses['G'].append(g_loss.data[0])\n",
    "\n",
    "#     def _gradient_penalty(self, real_data, generated_data):\n",
    "#         batch_size = real_data.size()[0]\n",
    "\n",
    "#         # Calculate interpolation\n",
    "#         alpha = torch.rand(batch_size, 1, 1, 1)\n",
    "#         alpha = alpha.expand_as(real_data)\n",
    "#         if self.use_cuda:\n",
    "#             alpha = alpha.cuda()\n",
    "#         interpolated = alpha * real_data.data + (1 - alpha) * generated_data.data\n",
    "#         interpolated = Variable(interpolated, requires_grad=True)\n",
    "#         if self.use_cuda:\n",
    "#             interpolated = interpolated.cuda()\n",
    "\n",
    "#         # Calculate probability of interpolated examples\n",
    "#         prob_interpolated = self.D(interpolated)\n",
    "\n",
    "#         # Calculate gradients of probabilities with respect to examples\n",
    "#         gradients = torch_grad(outputs=prob_interpolated, inputs=interpolated,\n",
    "#                                grad_outputs=torch.ones(prob_interpolated.size()).cuda() if self.use_cuda else torch.ones(\n",
    "#                                prob_interpolated.size()),\n",
    "#                                create_graph=True, retain_graph=True)[0]\n",
    "\n",
    "#         # Gradients have shape (batch_size, num_channels, img_width, img_height),\n",
    "#         # so flatten to easily take norm per example in batch\n",
    "#         gradients = gradients.view(batch_size, -1)\n",
    "#         self.losses['gradient_norm'].append(gradients.norm(2, dim=1).mean().data[0])\n",
    "\n",
    "#         # Derivatives of the gradient close to 0 can cause problems because of\n",
    "#         # the square root, so manually calculate norm and add epsilon\n",
    "#         gradients_norm = torch.sqrt(torch.sum(gradients ** 2, dim=1) + 1e-12)\n",
    "\n",
    "#         # Return gradient penalty\n",
    "#         return self.gp_weight * ((gradients_norm - 1) ** 2).mean()\n",
    "\n",
    "#     def _train_epoch(self, data_loader):\n",
    "#         for i, data in enumerate(data_loader):\n",
    "#             self.num_steps += 1\n",
    "#             self._critic_train_iteration(data[0])\n",
    "#             # Only update generator every |critic_iterations| iterations\n",
    "#             if self.num_steps % self.critic_iterations == 0:\n",
    "#                 self._generator_train_iteration(data[0])\n",
    "\n",
    "#             if i % self.print_every == 0:\n",
    "#                 print(\"Iteration {}\".format(i + 1))\n",
    "#                 print(\"D: {}\".format(self.losses['D'][-1]))\n",
    "#                 print(\"GP: {}\".format(self.losses['GP'][-1]))\n",
    "#                 print(\"Gradient norm: {}\".format(self.losses['gradient_norm'][-1]))\n",
    "#                 if self.num_steps > self.critic_iterations:\n",
    "#                     print(\"G: {}\".format(self.losses['G'][-1]))\n",
    "\n",
    "#     def train(self, data_loader, epochs, save_training_gif=True):\n",
    "#         if save_training_gif:\n",
    "#             # Fix latents to see how image generation improves during training\n",
    "#             fixed_latents = Variable(self.G.sample_latent(64))\n",
    "#             if self.use_cuda:\n",
    "#                 fixed_latents = fixed_latents.cuda()\n",
    "#             training_progress_images = []\n",
    "\n",
    "#         for epoch in range(epochs):\n",
    "#             print(\"\\nEpoch {}\".format(epoch + 1))\n",
    "#             self._train_epoch(data_loader)\n",
    "\n",
    "#             if save_training_gif:\n",
    "#                 # Generate batch of images and convert to grid\n",
    "#                 img_grid = make_grid(self.G(fixed_latents).cpu().data)\n",
    "#                 # Convert to numpy and transpose axes to fit imageio convention\n",
    "#                 # i.e. (width, height, channels)\n",
    "#                 img_grid = np.transpose(img_grid.numpy(), (1, 2, 0))\n",
    "#                 # Add image grid to training progress\n",
    "#                 training_progress_images.append(img_grid)\n",
    "\n",
    "#         if save_training_gif:\n",
    "#             imageio.mimsave('./training_{}_epochs.gif'.format(epochs),\n",
    "#                             training_progress_images)\n",
    "\n",
    "#     def sample_generator(self, num_samples):\n",
    "#         latent_samples = Variable(self.G.sample_latent(num_samples))\n",
    "#         if self.use_cuda:\n",
    "#             latent_samples = latent_samples.cuda()\n",
    "#         generated_data = self.G(latent_samples)\n",
    "#         return generated_data\n",
    "\n",
    "#     def sample(self, num_samples):\n",
    "#         generated_data = self.sample_generator(num_samples)\n",
    "#         # Remove color channel\n",
    "#         return generated_data.data.cpu().numpy()[:, 0, :, :]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
